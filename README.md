# LLMs_bascis

Topics related to Large Language Models (LLMs) and Transformers:

## Fundamental Architecture

* Transformer Architecture Overview
* Multi-head Attention Mechanisms
* Generalized Query Attention (GQA)
* Mixture of Experts (MOE)

## Core Components

* Tokenization Methods
* Position Embeddings
  + Traditional Position Embedding (PE)
  + Rotary Position Embedding (RoPE)
* Layer Normalization
* Decoding Strategies

## Model Training and Fine-tuning
* LoRA (Low-Rank Adaptation)
* Reinforcement Learning Approaches
  + RLHF (Reinforcement Learning from Human Feedback)
  + PPO (Proximal Policy Optimization)
  + DPO (Direct Preference Optimization)

## Optimization Techniques

* Rejection Sampling
* Inference Acceleration Methods
* Model Compression Strategies
  
